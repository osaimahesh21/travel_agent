class LLMService:
    """
    Encapsulates all interactions with the LLM model (Ollama / Mistral).

    Responsibilities:
    - Accept prompt text
    - Send to LLM API
    - Return the generated response
    - Decouple AI calls from higher-level services
    """

    def __init__(self, base_url: str, model_name: str):
        self.base_url = base_url
        self.model_name = model_name

    def generate(self, prompt: str, max_tokens: int = 512) -> str:
        """
        Send a prompt to the LLM and return the generated response.

        Parameters:
        - prompt: The input text to send to the model
        - max_tokens: Optional limit on the number of tokens in the response

        Returns:
        - The text generated by the model
        """

        # For now, we'll return a dummy response for testing purposes.
        # In a real implementation, this method would make an HTTP request
        # to the Ollama API and handle the response accordingly.

        return f"Generated response for prompt: {prompt[:50]}..."